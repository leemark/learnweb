---
layout: guide-part
title: "Part 4: Integrating AI APIs & Services - LearnWeb"
description: "Learn how to integrate AI APIs like OpenAI, Anthropic, and others into your web applications"
subpage: true
guide_title: "AI for Web Developers"
tagline: "Part 4: Integrating AI APIs & Services"
course_id: "ai"
lesson_id: "part-4"
course_url: "index.html"
next_lesson:
  title: "Next Part: AI Tools for Development Workflow"
  url: "part-5.html"
---
<!-- Breadcrumb Navigation -->
<nav class="breadcrumb" aria-label="Breadcrumb">
    <ol class="breadcrumb-list">
        <li class="breadcrumb-item"><a href="/">Home</a></li>
        <li class="breadcrumb-item"><a href="/ai/">AI for Web Developers</a></li>
        <li class="breadcrumb-item">Part 4: Integrating AI APIs & Services</li>
    </ol>
</nav>

<!-- Lesson Meta Information -->
<div class="lesson-meta">
    <div class="lesson-meta-item">
        <span class="material-symbols-outlined">article</span>
        <span>Part 4 of 6</span>
    </div>
    <!-- Reading time will be auto-calculated by JavaScript -->
</div>

<h2>Choosing an AI API Provider</h2>

<p>Most web developers integrate AI through APIs rather than building models from scratch. Several providers offer powerful AI capabilities you can access with a few lines of code.</p>

<h3>Major AI API Providers</h3>

[Diagram: AI API Provider Comparison. Four major providers: (1) OpenAI (GPT-4, GPT-4o) - most popular, great docs, vision/audio support, can be expensive. Best for: general purpose. (2) Anthropic (Claude 3.5 Sonnet) - excellent reasoning, 200K context, strong at code, newer with less docs. Best for: complex tasks. (3) Google (Gemini Pro) - multimodal, free tier, fast, has rate limits. Best for: prototyping. (4) Open Source (Llama, Mistral) - full control, privacy, no API costs, hosting complexity. Best for: privacy needs. Also shows Specialized Services: Hugging Face (thousands of models), Cohere (embeddings/search), Stability AI (image generation), Replicate (open source via API), ElevenLabs (text-to-speech), AssemblyAI (speech-to-text).]

<h2>Getting Started with OpenAI API</h2>

<p>Let's walk through a complete example using OpenAI's API (the most popular choice).</p>

<h3>Step 1: Get API Key</h3>

<ol>
  <li>Sign up at <code>platform.openai.com</code></li>
  <li>Navigate to API keys section</li>
  <li>Create a new API key</li>
  <li>Store it securely (never commit to GitHub!)</li>
</ol>

<h3>Step 2: Install SDK</h3>

<pre><code># Node.js
npm install openai

# Python
pip install openai</code></pre>

<h3>Step 3: Basic Implementation</h3>

<pre><code>// Node.js/JavaScript example
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY, // Store in environment variable
});

async function generateResponse(userMessage) {
  try {
    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "system",
          content: "You are a helpful customer support assistant."
        },
        {
          role: "user",
          content: userMessage
        }
      ],
      temperature: 0.7,
      max_tokens: 500,
    });

    return completion.choices[0].message.content;
  } catch (error) {
    console.error('OpenAI API error:', error);
    throw error;
  }
}

// Usage
const response = await generateResponse("How do I reset my password?");
console.log(response);</code></pre>

<h3>Key Parameters Explained</h3>

<ul>
  <li><strong>model</strong> – Which AI model to use (gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.)</li>
  <li><strong>messages</strong> – Array of conversation messages with roles (system, user, assistant)</li>
  <li><strong>temperature</strong> – Randomness/creativity (0 = deterministic, 2 = very creative)</li>
  <li><strong>max_tokens</strong> – Maximum length of response (limits cost)</li>
  <li><strong>top_p</strong> – Alternative to temperature for controlling randomness</li>
  <li><strong>presence_penalty</strong> – Encourages talking about new topics (-2 to 2)</li>
  <li><strong>frequency_penalty</strong> – Reduces repetition (-2 to 2)</li>
</ul>

<h2>Architecture Patterns for AI Integration</h2>

<h3>Pattern 1: Client-Side Direct (Simple but Limited)</h3>

[Diagram: Client-Side Direct AI Integration (NOT RECOMMENDED). Shows: Browser/User App → direct connection to OpenAI API. Warning: "API key exposed!" - Security risk! This pattern exposes your API key in client code.]

<p><strong>Why avoid:</strong> Your API key is exposed in client code, allowing anyone to use (and abuse) your quota.</p>

<h3>Pattern 2: Backend Proxy (Recommended)</h3>

[Diagram: Backend Proxy Architecture (RECOMMENDED). Shows: Frontend/Browser → User request → Your Backend (Node/Python) → API call (API key secure) → AI API (OpenAI/etc). Backend handles: Auth user, Rate limit, Validate input. Result: Secure! This pattern keeps your API key protected on the server.]

<p><strong>Implementation example:</strong></p>

<pre><code>// Backend API endpoint (Express.js)
import express from 'express';
import OpenAI from 'openai';

const app = express();
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

app.post('/api/chat', async (req, res) => {
  // 1. Authenticate user
  const user = await authenticateUser(req);
  if (!user) {
    return res.status(401).json({ error: 'Unauthorized' });
  }

  // 2. Rate limiting
  if (await isRateLimited(user.id)) {
    return res.status(429).json({ error: 'Too many requests' });
  }

  // 3. Validate and sanitize input
  const { message } = req.body;
  if (!message || message.length > 1000) {
    return res.status(400).json({ error: 'Invalid message' });
  }

  try {
    // 4. Call OpenAI API
    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: message }
      ],
      max_tokens: 500,
    });

    // 5. Log usage for billing
    await logUsage(user.id, completion.usage);

    // 6. Return response
    res.json({
      response: completion.choices[0].message.content,
      usage: completion.usage
    });
  } catch (error) {
    console.error('OpenAI error:', error);
    res.status(500).json({ error: 'AI service unavailable' });
  }
});

// Frontend code
async function askAI(message) {
  const response = await fetch('/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message }),
  });

  if (!response.ok) throw new Error('Request failed');
  return response.json();
}</code></pre>

<h3>Pattern 3: Streaming Responses</h3>

<p>For better UX, stream AI responses token-by-token instead of waiting for the complete response.</p>

<pre><code>// Backend - streaming endpoint
app.post('/api/chat/stream', async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');

  const stream = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [{ role: "user", content: req.body.message }],
    stream: true,
  });

  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content || '';
    if (content) {
      res.write(`data: ${JSON.stringify({ content })}\n\n`);
    }
  }

  res.write('data: [DONE]\n\n');
  res.end();
});

// Frontend - receive streaming response
async function streamAIResponse(message) {
  const response = await fetch('/api/chat/stream', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message }),
  });

  const reader = response.body.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { value, done } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split('\n');

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6);
        if (data === '[DONE]') return;

        const parsed = JSON.parse(data);
        updateUI(parsed.content); // Update UI incrementally
      }
    }
  }
}</code></pre>

<h2>Cost Management Strategies</h2>

<p>AI API costs can add up quickly. Implement these strategies to control expenses:</p>

<h3>1. Caching</h3>

<pre><code>import Redis from 'ioredis';
const redis = new Redis();

async function getCachedAIResponse(prompt) {
  // Check cache first
  const cached = await redis.get(`ai:${hash(prompt)}`);
  if (cached) return JSON.parse(cached);

  // Call AI if not cached
  const response = await openai.chat.completions.create({...});
  const result = response.choices[0].message.content;

  // Cache for 1 hour
  await redis.setex(`ai:${hash(prompt)}`, 3600, JSON.stringify(result));

  return result;
}</code></pre>

<h3>2. Token Limits & Truncation</h3>

<pre><code>import { encoding_for_model } from 'tiktoken';

function truncateToTokenLimit(text, maxTokens = 4000) {
  const encoding = encoding_for_model('gpt-4');
  const tokens = encoding.encode(text);

  if (tokens.length <= maxTokens) return text;

  // Truncate and decode back to text
  const truncated = tokens.slice(0, maxTokens);
  return encoding.decode(truncated);
}</code></pre>

<h3>3. Model Selection</h3>

<pre><code>// Use cheaper models for simple tasks
function chooseModel(taskComplexity) {
  if (taskComplexity === 'simple') {
    return 'gpt-3.5-turbo'; // $0.0005 per 1K tokens
  } else {
    return 'gpt-4o'; // $0.0025-$0.01 per 1K tokens
  }
}</code></pre>

<h3>4. Rate Limiting</h3>

<pre><code>import rateLimit from 'express-rate-limit';

const aiRateLimiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 50, // Limit each user to 50 requests per window
  message: 'Too many AI requests, please try again later.',
});

app.post('/api/chat', aiRateLimiter, async (req, res) => {
  // ... handle request
});</code></pre>

<h2>Error Handling Best Practices</h2>

<pre><code>async function robustAICall(prompt, options = {}) {
  const maxRetries = 3;
  let lastError;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      const response = await openai.chat.completions.create({
        ...options,
        messages: [{ role: "user", content: prompt }],
      });

      return response.choices[0].message.content;

    } catch (error) {
      lastError = error;

      // Don't retry on certain errors
      if (error.status === 400) {
        throw new Error('Invalid request: ' + error.message);
      }

      // Retry on rate limits with exponential backoff
      if (error.status === 429) {
        const delay = Math.pow(2, attempt) * 1000;
        console.log(`Rate limited, retrying in ${delay}ms...`);
        await new Promise(resolve => setTimeout(resolve, delay));
        continue;
      }

      // Retry on server errors
      if (error.status >= 500) {
        console.log(`Server error, retrying attempt ${attempt + 1}...`);
        await new Promise(resolve => setTimeout(resolve, 1000));
        continue;
      }

      throw error;
    }
  }

  throw new Error(`Failed after ${maxRetries} attempts: ${lastError.message}`);
}</code></pre>

<h3>Key Takeaways</h3>

<ul>
  <li>Major AI providers: OpenAI (most popular), Anthropic (best reasoning), Google (free tier), open source (privacy/control).</li>
  <li>Never expose API keys in client code—always use a backend proxy.</li>
  <li>Backend proxy pattern: frontend → your server → AI API (enables auth, rate limiting, input validation).</li>
  <li>Stream responses for better UX—show text as it's generated instead of waiting.</li>
  <li>Manage costs: cache responses, set token limits, choose cheaper models for simple tasks, implement rate limiting.</li>
  <li>Robust error handling: retry with exponential backoff for rate limits, don't retry on client errors (400s).</li>
  <li>Key parameters: model, temperature (randomness), max_tokens (cost control), messages (conversation context).</li>
  <li>Monitor usage and set budget alerts to avoid surprise bills.</li>
  <li>Log all AI requests for debugging, analytics, and compliance.</li>
  <li>Implement timeouts to prevent hanging requests.</li>
</ul>

<p>Next, let's explore AI-powered development tools that can accelerate your workflow.</p>

<!-- Mark as Complete -->
<div class="lesson-completion">
    <label class="lesson-completion-label">
        <div class="lesson-completion-checkbox">
            <span class="material-symbols-outlined">check</span>
        </div>
        <span class="lesson-completion-text">Mark this lesson as complete</span>
    </label>
</div>
